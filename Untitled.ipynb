{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We use class to describe each sample, each sample means each abstract\n",
    "'''\n",
    "class Sample():\n",
    "    def __init__(self, row):\n",
    "        self.ID = row['Id']\n",
    "        self.Title = row['Title']\n",
    "        self.Abstract = row['Abstract']\n",
    "        self.Authors = row['Authors']\n",
    "        self.Categories = row['Categories']\n",
    "        self.Date = row['Created Date']\n",
    "        self.label = row['Task 1']\n",
    "        self.dict = self.get_dict()\n",
    "        self.sentences = self.get_sentences()\n",
    "        \n",
    "    def get_dict(self):\n",
    "        abstract_list = self.Abstract.split('$$$')\n",
    "        label_list = self.label.split(' ')\n",
    "        DICT = {}\n",
    "        for i in range(len(label_list)):\n",
    "            DICT[abstract_list[i]] = label_list[i]\n",
    "        return DICT\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        return self.Abstract.split('$$$')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    tfidf\n",
      "brain            0.354353\n",
      "multichannel     0.285744\n",
      "neuroscientists  0.285744\n",
      "diagnose         0.267158\n",
      "treatments       0.244438\n",
      "multilevel       0.239232\n",
      "permits          0.225852\n",
      "devise           0.220646\n",
      "diseases         0.218379\n",
      "collect          0.202860\n",
      "rapid            0.194247\n",
      "popularity       0.191988\n",
      "things           0.191719\n",
      "iot              0.177333\n",
      "understand       0.172081\n",
      "internet         0.170094\n",
      "cloud            0.162082\n",
      "computing        0.149882\n",
      "functions        0.146301\n",
      "better           0.138674\n",
      "and              0.138262\n",
      "to               0.096733\n",
      "data             0.090834\n",
      "of               0.082537\n",
      "pfgs             0.000000\n",
      "pgql             0.000000\n",
      "ph2              0.000000\n",
      "ph               0.000000\n",
      "pgrd             0.000000\n",
      "pfcvmlp          0.000000\n",
      "...                   ...\n",
      "existential      0.000000\n",
      "existent         0.000000\n",
      "existence        0.000000\n",
      "expands          0.000000\n",
      "expanses         0.000000\n",
      "expansion        0.000000\n",
      "expeditious      0.000000\n",
      "experi           0.000000\n",
      "expensiveness    0.000000\n",
      "expensively      0.000000\n",
      "expensive        0.000000\n",
      "expension        0.000000\n",
      "expenses         0.000000\n",
      "expense          0.000000\n",
      "expenditure      0.000000\n",
      "expelled         0.000000\n",
      "expeditiously    0.000000\n",
      "expeditions      0.000000\n",
      "expansions       0.000000\n",
      "expedites        0.000000\n",
      "expedite         0.000000\n",
      "expedient        0.000000\n",
      "expects          0.000000\n",
      "expected         0.000000\n",
      "expectations     0.000000\n",
      "expectation      0.000000\n",
      "expect           0.000000\n",
      "expatiated       0.000000\n",
      "expansive        0.000000\n",
      "zynq             0.000000\n",
      "\n",
      "[27187 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "if __name__ == '__main__':\n",
    "    train_df = pd.read_csv(r'C:\\Users\\ratana\\Desktop\\Data mining project\\task1_trainset.csv') # read csv as pandas dataframe\n",
    "    \n",
    "    # instanciate each object\n",
    "    Sample_obj = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        Sample_obj.append(Sample(row))\n",
    "        \n",
    "    # Get the sentences from each object(each sample)\n",
    "    Abs_list = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        Abs_list.extend(Sample_obj[index].get_sentences())\n",
    "        \n",
    "    # implement with sklearn tfidf tool    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(Abs_list)\n",
    "    \n",
    "    # tfidf_vectorizer_vectors has a dimension of n_samples*n_features\n",
    "    # We can take the first sample and transform it into dataframe to investigate what it is\n",
    "    first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n",
    "    df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=['tfidf'])\n",
    "    df.sort_values(by=['tfidf'],ascending=False, inplace=True)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
